{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ead8e-1caf-4425-ba57-d8f3ac25d3e5",
   "metadata": {},
   "source": [
    "## What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ed8bb1-5d4e-4c4f-8723-45338e52c1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data. A single machine learning model might make prediction errors depending on the accuracy of the training dataset. For example, if a cat-identifying model has been trained only on images of white cats, it may occasionally misidentify a black cat. Boosting tries to overcome this issue by training multiple models sequentially to improve the accuracy of the overall system.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data. A single machine learning model might make prediction errors depending on the accuracy of the training dataset. For example, if a cat-identifying model has been trained only on images of white cats, it may occasionally misidentify a black cat. Boosting tries to overcome this issue by training multiple models sequentially to improve the accuracy of the overall system.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffed43f-5534-40dd-84a8-448b4aaa584f",
   "metadata": {},
   "source": [
    "##  Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b5dfc6-df8e-401f-a669-c834cb6e2bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe key benefits of boosting include:\\nEase of Implementation: Boosting can be used with several hyper-parameter tuning options to improve fitting. ...\\nReduction of bias: Boosting algorithms combine multiple weak learners in a sequential method, iteratively improving upon observations\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The key benefits of boosting include:\n",
    "Ease of Implementation: Boosting can be used with several hyper-parameter tuning options to improve fitting. ...\n",
    "Reduction of bias: Boosting algorithms combine multiple weak learners in a sequential method, iteratively improving upon observations\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5781f26-edb1-40e0-a241-136dd4015c9c",
   "metadata": {},
   "source": [
    "## Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5f86d6-1ff3-463f-b07c-1797c51f7345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3eaf4c-1bc9-45bd-b3fb-d7cbf7eb98ba",
   "metadata": {},
   "source": [
    "##  Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464421f4-60c0-49d6-a4e3-9450a34bc6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are three types of Boosting Algorithms which are as follows:\\nAdaBoost (Adaptive Boosting) algorithm.\\nGradient Boosting algorithm.\\nXG Boost algorithm.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "There are three types of Boosting Algorithms which are as follows:\n",
    "AdaBoost (Adaptive Boosting) algorithm.\n",
    "Gradient Boosting algorithm.\n",
    "XG Boost algorithm.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6dd1d-25b7-4ab2-a2f5-013e9d95cd60",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5825d633-d819-40c4-91c1-8af8286f01bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs discussed earlier, there are two types of parameter to be tuned here – tree based and boosting parameters. There are no optimum values for learning rate as low values always work better, given that we train on sufficient number of trees\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "As discussed earlier, there are two types of parameter to be tuned here – tree based and boosting parameters. There are no optimum values for learning rate as low values always work better, given that we train on sufficient number of trees\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843042ee-90e0-4788-a7b3-08d420995367",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1487bfda-4973-4b9f-86ae-509911a0ac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each model tries to compensate for the weaknesses of its predecessor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23a9b2-e5bc-4b14-bcc1-2085c56a2e02",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd9344b-52dd-4211-94bb-4fd2fdd9d644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method. The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. These trees are also called Decision Stumps.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "AdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method. The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. These trees are also called Decision Stumps.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d18ac2-4c9d-4c16-bd44-7e0feee51b11",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
