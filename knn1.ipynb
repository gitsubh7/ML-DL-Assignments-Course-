{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4261e2fc-37f7-452e-9077-ae022b24a4b7",
   "metadata": {},
   "source": [
    "## Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44a3ea4-f06d-4f60-a544-eb7d117d038a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414d6b5-d125-411e-9c8e-544dcf5006f0",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c5d439-2d5f-4c8f-8e35-f84a6ad7afc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a828598e-98df-42dd-bf96-cf146c4265ba",
   "metadata": {},
   "source": [
    "## Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4976050b-54a5-4a7b-981d-d5be697856ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe difference between the KNN classifier and KNN regression methods is that the classifier is used in situations where the response variable is categorical (qualitative), while the regressor is used in numerical situations (quantitative)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The difference between the KNN classifier and KNN regression methods is that the classifier is used in situations where the response variable is categorical (qualitative), while the regressor is used in numerical situations (quantitative)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a471a57-d772-4447-aafa-6beea4827800",
   "metadata": {},
   "source": [
    "## Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d7b72f-657b-45f2-af4e-2a1b32a0c774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe k-nearest neighbour classification (k-NN) is one of the most popular distance-based algorithms. This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output. The traditional k-NN classifier works naturally with numerical data\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The k-nearest neighbour classification (k-NN) is one of the most popular distance-based algorithms. This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output. The traditional k-NN classifier works naturally with numerical data\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a54ba-7ad8-4a26-8f4c-338e35b2eb3c",
   "metadata": {},
   "source": [
    "## Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0189c457-81c4-443c-bc21-0a8418f9a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f998e5-a452-4834-87e3-7cd8d569e3f0",
   "metadata": {},
   "source": [
    "## Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c1ccce5-f555-4704-8df7-a21d7f4bff7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65e228-f735-4463-8ad9-f2b3598aec70",
   "metadata": {},
   "source": [
    "## Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249a2972-1033-47e7-b37a-6e6c3bbea08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5478e88a-48f7-4025-abfc-043567fd642a",
   "metadata": {},
   "source": [
    "## Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1f308c-3b6e-48b8-b6b8-299d9b60b221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nt has advantages - nonparametric architecture, simple and powerful, requires no traning time, but it also has disadvantage - memory intensive, classification and estimation are slow'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "t has advantages - nonparametric architecture, simple and powerful, requires no traning time, but it also has disadvantage - memory intensive, classification and estimation are slow'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811230e1-cfeb-4d48-9e81-345a72780342",
   "metadata": {},
   "source": [
    "## Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eef7a4f-750e-4658-a5fa-29761a47f0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEuclidean distance is the shortest path between source and destination which is a straight line as shown in Figure 1.3. but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines as shown in Figure 1.4\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Euclidean distance is the shortest path between source and destination which is a straight line as shown in Figure 1.3. but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines as shown in Figure 1.4\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f83d60-2e4d-4fbf-80f2-e8097ad4c151",
   "metadata": {},
   "source": [
    "## Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e1805-6d9a-4f63-ab29-ac2499ea7799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
